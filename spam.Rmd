---
title: KNN - SVM
subtitle: Ejercicio Obligatorio
author:
- name: William Chavarría
  affiliation: Máxima Formación
  email: wchavarria@tigo.com.gt
date: '`r format(Sys.Date())`'
output: 
  bookdown::html_document2:
    toc: true
    toc_float: true
    highlight: pygments
    theme: spacelab
    css: custom_knn.css
    fig_caption: true
    df_print: paged
bibliography: [paquetes_knn.bib, knn.bib]
biblio-style: "apalike"
link-citations: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo        = TRUE, 
                      include     = TRUE,
                      cache       = FALSE,
                      fig.align   = 'center',
                      message     = FALSE,
                      warning     = FALSE, 
                      comment     = NA, 
                      highlight   = TRUE,
                      strip.white = TRUE,
                      dev         = "svglite",
                      fig.width   = 8,
                      fig.asp     = 0.618,
                      fig.show    = "hold",
                      fig.align   = "center")
```

# spam {.tabset .tabset-fade .tabset-pills}

## Descripción

Datos spam. Se desea realizar un modelo para clasificar si un correo se puede
considerar spam o no. Se nos proporciona un conjunto de datos con 4.601
observaciones y 58 variables, donde la variable type es la variable a predecir.

Realice los siguientes pasos:

1.  Realice el proceso de pre-procesado. Divida el conjunto de datos en 70%
    para el conjunto de entrenamiento y 30% para el conjunto de validación.

2.  Realice un modelo de K vecinos más cercanos con la K óptima. Pruebe con 20
    valores distintos de k y obtenga la evolución del Accuracy en función de la
    K, seleccionando la k óptima. Utilice el modelo para predecir sobre el
    conjunto de validación.

3.  Realice un modelo de Super Vector Machine con distintos kernels (lineal,
    polinómico y radial). Utilice el modelo para predecir sobre el conjunto de
    validación

4.  Obtenga el AUC del modelo de Knn y SVM ¿Cuál de los dos modelos podemos
    considerar que es mejor en función del AUC? (para calcular el AUC deberá
    transformar las predicciones a numéricas con la función ifelse()).

Interprete los resultados. Los datos se encuentran dentro de la librería
kernlab que debemos instalar (install.packages()) y cargar (library()). Para
cargar los datos se debe utilizar data(spam).

Descripción del conjunto de datos:

-   Conjunto de datos recopilados en Hewlett-Packard Labs, que clasifica los
    correos electrónicos 4601 como spam o no spam. Las primeras 57 variables
    nos indican la frecuencia de ciertas palabras y caracteres en el correo
    electrónico y la variable número 58 (type) clasifica el correo como spam o
    no.

## Paquetes

```{r}
options(warn = -1,
		  dplyr.summarise.inform = FALSE,
		  tibble.print_min = 5,
		  readr.show_col_types = FALSE)
```

```{r}
import::from(statistigo, coloring_font)
import::from(finetune, control_race)
import::from(statistigo, coloring_font)
import::from(skimr, skim)
import::from(patchwork, plot_layout, plot_annotation)
import::from(caret, nearZeroVar)
import::from(parallel, detectCores, makePSOCKcluster, stopCluster)
import::from(tidytext, reorder_within, scale_y_reordered, scale_x_reordered)
import::from(doParallel, registerDoParallel)
import::from(cowplot, .except = "stamp")
import::from(kableExtra, .except = "group_rows")
import::from(magrittr, "%T>%", "%$%", .into = "operadores")
import::from(DataExplorer, plot_intro, plot_bar, plot_density)
import::from(conectigo, cargar_fuentes)
import::from(colorblindr, scale_color_OkabeIto)
pacman::p_load(formattable, janitor, pins, themis, usemodels, tidymodels, tidyverse)
```

## Funciones

```{r}
tabla <- function(df, cap = "prueba") {
  
  df %>% 
   kbl(booktabs = TRUE, caption = cap, escape = F) %>% 
   kable_paper(lightable_options = "hover", full_width = F)}
```

```{r}
resaltar <- function(texto) {
    
    glue::glue("<span style='background-color: #FFFF00'>**{texto}**</span>")
    
}
```

```{r}
# detener el backend
unregister <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
```

<!-- #56B4E9 -->

<!-- #D55E00 -->

```{r}
barra <- function(df, x) {
	
	dfx <- df %>%
		tabyl({{x}}) %>% 
		adorn_pct_formatting()
	
	dfx %>% 
		ggplot(aes(y = {{x}}, x = n)) +
		geom_col(fill = "#0072B2", width = 0.8) +
		geom_text(aes(label = str_c(n, " ", "(", percent, ")")),
					 hjust = 1.5,
					 size = 6,
					 color = "white") +
					 # fontface = "bold") +
		scale_x_continuous(name = NULL, expand = c(0, 0)) +
		scale_y_discrete(name = NULL, expand = c(0, 0.5)) +
		# coord_cartesian(clip = "off") +
		theme_minimal_vgrid(font_family = "yano") +
		theme(axis.text.y = element_text(size = 14),
				plot.title = element_text(size = 22, face = "bold"))
}
```

## Opciones

```{r}
set.seed(2021)
```


```{r}
cargar_fuentes()
```

```{r}
yunkel <- theme_cowplot(font_family = "yano") +
	       theme(plot.margin = unit(c(3, 1, 1, 1), "mm"), 
	             axis.title = element_text(size = 12))
```

```{r}
# tema con grid horizontal y vertical
drako <- theme_bw(base_family = "yano", base_size = 14) +
	      theme(plot.margin = unit(c(6, 1, 1, 1), "mm"),
	            axis.title = element_text(size = 12),
	            plot.subtitle = element_text(size = 8,
                                            family = "sans"))
```

```{r}
theme_set(yunkel)
```

# Carga

Cargar los datos y transformar aquellas variables que se consideren factor.

```{r}
spam <- data("spam", package = "kernlab") %>%
	get() %>% as_tibble(.name_repair = make_clean_names)
```

<!-- https://bit.ly/31D95c4 -->


# Análisis Exploratorio

## Estructura

```{r}
head(spam)
```

<br/>

```{r}
plot_intro(spam, ggtheme = yunkel)
```

<br/>

Ninguna de nuestras variables tiene datos ausentes. Solo hay una variable
categórica.

```{r}
spam %>%
	skim() %>%
	as_tibble() %>%
	select(skim_variable, factor.top_counts:numeric.sd)
```

Vemos que la variable respuesta presenta un desbalance con 2788 casos para
`r coloring_font("**no_spam**", "#A24000")` y 1813 para
`r coloring_font("**spam**", "#A24000")`.

## Variables dependiente

(ref:desbalance) El desafío de trabajar con conjuntos de datos desequilibrados es que la mayoría de las técnicas de aprendizaje automático ignorarán y, a su vez, tendrán un rendimiento deficiente en la clase minoritaria, aunque normalmente lo más importante es el rendimiento en la clase minoritaria.

```{r, desbalance, fig.cap='(ref:desbalance)'}
spam %>% 
	barra(type) +
	labs(title = "Clasificación desequilibrada")
```

<br/>

En el gráfico \@ref(fig:desbalance) observamos que la variable no está
equilibrada. La clase `r coloring_font("**no_spam**", "#A24000")` tiene más
casos que la clase `r coloring_font("**spam**", "#A24000")`. **Si no
balanceamos los datos entonces lo que pasará es que nuestro modelo aprenderá de
manera muy eficaz sobre cómo predecir el caso negativo, es decir, cuando un
correo no es spam.**

## Atípicos

Evaluemos la presencia de atípicos. Para esto primero excluiremos aquellas
columna que tengan varianza próxima a cero y solo dejaremos variables que
aporten suficiente información al modelo.

```{r}
var_ok <- spam %>% 
	nearZeroVar(saveMetrics = T) %>% 
	rownames_to_column("var") %>% 
    filter(!if_any(zeroVar:nzv, ~ .x == TRUE)) %>% 
    pull(var)
```

Debido a que las variables están en diferentes magnitudes y lo que nos interesa
es evaluar la presencia de atípicos, realizaremos una transformación a los
valores utilizando la **transformación del logaritmo ajustado.: ** $log(Y + 1)$.
De esta forma los valores con 1 se convertirán en cero al aplicar el logaritmo.

```{r}
sl <- spam %>% 
	select(all_of(var_ok)) %>% 
	mutate(across(is.numeric, ~ log(.x + 1)))
```

```{r}
slice_sample(sl, n = 5)
```

Para graficar no será suficiente la transformación logarítmica, así que
aplicaremos una segunda transformación a nivel del eje Y. Esto nos permitirá
ver de forma ordenada las variables con información que tienen atípicos
separada por cada uno de los distintos tipos de correo.


```{r, fig.width=11, fig.asp=0.6}
sl %>% 
	pivot_longer(cols = where(is.numeric),
					 names_to = "variable",
					 values_to = "valor") %>% 
	ggplot(aes(reorder_within(variable, valor, type, fun = median), valor)) +
	geom_boxplot(outlier.color = "red") +
	scale_x_reordered(name = "Métrica") +
	scale_y_log10() +
	facet_grid(type ~ ., scales = "free", space = "free") +
	drako +
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = -0.1),
			axis.title.x = element_blank()) +
	labs(title        = "Atípicos con variables informativas", 
		 subtitle     = "Doble escala logarítmica")
```

<br/>

El tratamiento de atípicos puede mejorarse a través de una técnica llamada
*spatial sign* [@kuhn_applied_2013 pag. 71-72]

# Split

```{r}
spam_split <- initial_split(data = spam, strata = type, prop = 0.7)
spam_train <- training(spam_split)
spam_test  <- testing(spam_split)
```

```{r}
dim(spam_train) # 70% para entrenamiento
```

# Cross-Validación

Debido a que tenemos una muestra relativamente grande[^1] un CV con 10 K-fold
será suficiente para obtener **buenas propiedades de bias y varianza.**

[^1]: Definir que es grande y que es pequeño es difícil

```{r, paged.print = FALSE}
(spam_folds <- vfold_cv(spam_train, v = 10, strata = type))
```

# Preprocesamiento

Crearemos varias recetas para poder probar con los distintos métodos que ayudan
a corregir el desequilibrio entre las distintas clases.

Lo que haremos en general será:

-   Balancear los datos con distintos métodos.
-   Remover variables que tengan cero varianza o varianza próxima a cero.
-   Centrar y escalar
-   Solo en una receta aplicaremos tratamiento a los atípicos.

Antes de realizar este procedimiento, validemos cuantas variables quedarían
después de remover las variables tienen varianza cero o próxima a cero.

```{r}
pdr <- recipe(formula = type ~ ., data = spam_train) %>% 
  step_nzv(all_predictors()) %>%
  prep() %>% juice()
```

```{r}
slice_sample(pdr, n = 5)
```

```{r}
ncol(pdr) - 1 # menos la VD
```

Vemos que solo quedan 9 predictores.

Revisemos si al remover la varianza cercana a cero los resultados de las
funciones de {caret} y {tidymodels} son iguales:

```{r}
setdiff(names(pdr), names(sl))
```

Vemos que hay una diferencia con la función `caret::nearZeroVar()` en cuanto a
la cantidad de columnas que filtra versus `step_nzv()`. Al revisar la
documentación de ambas funciones vemos que los valores por defecto son los
mismos, así que la razón obedece a que `step_nzv()` se está aplicando al
conjunto de entrenamiento y la función de {caret} a todo el conjunto original
de datos.

Ahora veamos que pasa si solo queremos retirar aquellas que tengan
estrictamente varianza cero:

```{r}
recipe(formula = type ~ ., data = spam_train) %>% 
  step_zv(all_predictors()) %>%
  prep() %>% 
  summary()
```

**No hay ninguna variable que tenga varianza cero exacta.** Lo que haremos será
dejar una receta que contemple todos los predictores y seleccionaremos un único
método de balanceo, porque sino, tendríamos demasiadas recetas que probar.

## SMOTE

**Synthetic Minority Oversampling TEchnique**

SMOTE funciona seleccionando ejemplos que están cerca en el espacio de
funciones, dibujando una línea entre los ejemplos en el espacio de funciones y
dibujando una nueva muestra en un punto a lo largo de esa línea.

SMOTE primero selecciona una instancia de clase minoritaria a al azar y
encuentra sus k vecinos de clase minoritaria más cercanos. Luego, la instancia
sintética se crea eligiendo uno de los k vecinos b más cercanos al azar y
conectando a y b para formar un segmento de línea en el espacio de
características. Las instancias sintéticas se generan como una combinación
convexa de las dos instancias elegidas a y b.

```{r}
receta_smote <- recipe(formula = type ~ ., data = spam_train) %>% 
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric()) %>% 
  step_smote(type, skip = TRUE)
```

Veamos como quedó esta receta a nivel de dataset:

```{r}
receta_smote %>%
	prep() %>%
	juice() %>% 
	slice_sample(n = 4)
```

Vemos que, a como habíamos previsto, que la función `step_nzv()` **excluyó 48
predictores debido a que tienen varianza próxima a cero**, quedando únicamente 9
predictores más la respuesta.

(ref:smot) Realiza un sobre muestreo en el que la clase minoritaria se sobremuestrea creando ejemplos "sintéticos" en lugar de sobremuestrear con reemplazo.

```{r smot, fig.cap='(ref:smot)'}
receta_smote %>%
	prep() %>%
	juice() %>% 
	barra(type) + labs("Clasificación Balanceada")
```

<br/>

## SMOTE + Atípicos

Utilizaremos una receta con un paso adicional denominado *spatial sign* el
cual, según [@johnson_63_nodate]:

> "La transformación de signo espacial toma un conjunto de variables predictoras
y las transforma de manera que los nuevos valores tengan la misma distancia al
centro de la distribución. En esencia, los datos se proyectan en una esfera
multidimensional ..."

```{r}
receta_smote_outliers <- receta_smote %>% 
  step_spatialsign(all_predictors())
```

## Submuestreo

Recordemos que en tidymodels las recetas de preprocesamiento que se apliquen a
los datos de entrenamiento luego se aplican a los datos de prueba. En el caso
de pasos (*steps*) de pre-procesamiento que realizan sobre-muestreo o
sub-muestreo es muy importante que este paso **no se aplique a los datos que
estamos pronosticando**. Por esta razón cuando usemos recetas debemos utilizar
una opción llamada `skip = TRUE` para que se ignore este paso en la fase de
predicción (e.g con `predict()`).

La idea principal es aislar los pasos de pre-procesamiento que podrían causar
errores si se aplican a nuevas muestras (e.g set de prueba).

```{r}
receta_submuestreo <- recipe(formula = type ~ ., data = spam_train) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric()) %>% 
  step_downsample(type, skip = TRUE)
```

## Sobremuestreo

```{r}
receta_sobremuestreo <- recipe(formula = type ~ ., data = spam_train) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric()) %>% 
  step_upsample(type, skip = TRUE)
```

## ROSE

```{r}
receta_rose <- recipe(formula = type ~ ., data = spam_train) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric()) %>% 
  step_rose(type, skip = TRUE)
```

# Modelos

En este apartado definimos tres cosas: 

1. El modelo a utilizar
2. El modo. El cual puede ser clasificación o regresión.
3. El motor. En este caso la librería o paquete que contiene el modelo a utilizar.


## K-NN

Aquí *K* corresponde al parámetro *neighbors*. Establecemos `tune()` con el fin
de que encuentre la *K* óptima.

```{r}
knn_kknn <- nearest_neighbor(neighbors   = tune(),
						     weight_func = tune()) %>%
	set_mode("classification") %>%
	set_engine("kknn")
```

## SVM

Definiremos los distintos *kernels* a través de definiciones separadas.

- Lineal
- Polinomio
- Radial

```{r}
svm_l_kernlab <- svm_linear(cost = tune()) %>%
	set_mode("classification") %>%
	set_engine("kernlab")
```

```{r}
svm_p_kernlab <- svm_poly(cost   = tune(),
						  degree = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```


```{r}
svm_r_kernlab <- svm_rbf(cost = tune(), 
					     rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")
```

## Workflow

Agregaremos las recetas y los modelos a una lista nombrada para luego realizar
el tuning de hiperparámetros en una cuadrícula de 20 valores.

### Recetas

```{r}
preprocesadores <- list(smote_simple  = receta_smote, 
						smote_outlier = receta_smote_outliers, 
						submuestreo   = receta_submuestreo,
						sobremuestreo = receta_sobremuestreo,
						rose          = receta_rose)
```

Podemos aprovechar que tenemos esta lista de recetas para medir el impacto de
los algoritmos de balanceo en cuanto a la cantidad de filas que dejan para
ajustar el modelo

```{r observaciones, paged.print = FALSE}
preprocesadores %>%
	map(~ prep(.) %>%
		 	juice() %>% 
		 	nrow()) %>% 
	enframe() %>% 
	unnest(value) %>% 
	arrange(desc(value)) %>% 
	tabla(cap = "Cantidad de observaciones restantes")
```

<br/>

Con base a la tabla \@ref(tab:observaciones) vemos que el algoritmo de
**submuestreo resulta en menos observaciones** que los restantes tres,
pudiendo significar esto que hay perdida de información.

### Modelos

```{r}
modelos <- list(knn = knn_kknn,
			svm_p_kernlab   = svm_p_kernlab,
			svm_l_kernlab   = svm_l_kernlab,
			svm_r_kernlab   = svm_r_kernlab)
```

### Flujo

Esto es similar a crear una rejilla con la función de base `expand.grid()`. Se
realiza una combinación de todas las recetas con todos los modelos al establecer
el parámetro `cross = TRUE`.

Esto solo es posible debido a que ambos modelos (SVM y KNN) requieren
prácticamente los mismos pasos de preprocesamiento, aunque hay teorías que
indican que **SVM requiere que se eliminen colinealidades.** Para efectos
prácticos no realizaremos este paso en SVM.

```{r}
spam_workflow <- workflow_set(preproc = preprocesadores, 
							  models = modelos, 
							  cross = TRUE)
```

```{r, paged.print = FALSE}
spam_workflow
```

Tenemos ahora un objeto virgen (sin ajustar) que contiene todas las
combinaciones de recetas y modelos.

## Cuadrícula

Con la función `control_grid()` lo que haremos será retener los modelos y
recetas ajustados. Además, cuando establecemos la opción `save_pred = TRUE`
conservaremos las predicciones del conjunto de evaluación y podremos acceder a
ellas mediante `collect_predictions()` .

```{r}
grid_ctrl <- control_grid(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE)
```

## Métricas

Antes definamos el set de métricas para evaluar el modelo. En este caso,
adicional a la precisión general, el ROC_AUC, y el estadístico *kappa* (el cual
es un poco controversial[^2]) agregaremos la especificidad ya que con base a
[@kuhn_applied_2013 pag 347]

[^2]: Sobre el estadístico Kappa: [aqui](https://bit.ly/31WOsaL)

> "En el filtrado de correo no deseado, es probable que el costo de eliminar
> erróneamente un correo electrónico importante sea más alto que permitir que
> un correo electrónico no deseado pase el filtro incorrectamente. En
> situaciones donde los costos son diferentes, es posible que la precisión no
> mida las características importantes del modelo."

> "En el filtrado de spam, generalmente hay un enfoque en la **especificidad**;
> la mayoría de las personas están dispuestas a aceptar ver spam si no se
> eliminan los correos electrónicos de familiares o compañeros de trabajo."

Debido a que el motor de kernlab tiene la capacidad de generar predicciónes
continuas de tipo probabilidad, será posible combinar predicciones de clase
con estas últimas.

```{r}
mset <- metric_set(roc_auc, accuracy, kap, specificity, sensitivity)
```

## Tuning

Creamos un cluster para correr en paralelo.

```{r, eval=FALSE}
all_cores <- detectCores(logical = FALSE)
clusterpr <- makePSOCKcluster(all_cores)
registerDoParallel(clusterpr)
set.seed(2021)
```

```{r, echo=FALSE}
# corrida manual
# ruta <- fs::path_wd("06_svm_knn", "knn", "modelos")
# tablero <- board_folder(path = ruta)

# corrida en knit 
tablero <- board_folder(path = "./modelos")
```

```{r, echo=FALSE}
tune_res <- pin_read(board = tablero, name = "tune_res")
```

A continuación realizamos el ajuste de modelos y recetas, definidas en el objeto
`spam_workflow`, utilizando los remuestreos, el conjunto de métricas y una
cuadrícula de tamaño 20.

```{r, eval=FALSE}
tune_res <- spam_workflow %>% 
	workflow_map(fn       = "tune_grid", 
				verbose   = TRUE,
				resamples = spam_folds,
				control   = grid_ctrl,
				seed      = 2021,
				metrics   = mset,
				grid      = 20)
```

```{r, eval=FALSE}
stopCluster(clusterpr)
unregister()
```

```{r, paged.print = FALSE}
tune_res
```

<!-- Guardar un workflow con pins -->

```{r, eval=FALSE, echo=FALSE}
pin_write(board = tablero,
			 x     = tune_res,
			 name  = "tune_res",
			 type  = "rds",
			 title = "all_models",
			 description = "19575 modelos entrenados")
```

La columna `r coloring_font("**result**", "#A24000")` contiene  `tune[+]` lo
cual indica que todos los hiperparámetros fueron identificados correctamente.

```{r}
(tm <- nrow(collect_metrics(tune_res, summarize = FALSE)))
```

Se ajustaron un total de **`r tm`** modelos!

Ahora veamos los resultados de las métricas con nuestro conjunto de
entrenamiento.

```{r, paged.print = FALSE}
tune_rank <- tune_res %>%
	rank_results(select_best = TRUE, rank_metric = "specificity") %>% 
	select(modelo = wflow_id, .metric, mean, rank) %>% 
	pivot_wider(names_from = .metric, values_from = mean) %>% 
	rename(kappa = kap) %>% 
	relocate(specificity, .after = "rank")
```

```{r}
ranking_models <- tune_rank
ranking_models[, 3:7] <- map(tune_rank %>%
	 	select(is.double), ~ color_tile("#FC8D59", "lightgreen")(.x))
```



```{r, modelos, paged.print = FALSE}
ranking_models %>% 
	tabla(cap = "Ranking de los mejores ajustes con datos de entrenamiento")
```

<br/>

En la tabla \@ref(tab:modelos) vemos que el color más verde corresponde a la
métrica de esa columna que obtuvo la mayor puntuación y el color rojo el que 
obtuvo la menor.

Veamos esto mismo de forma gráfica.

(ref:all-models) Todas las combinaciones de modelos ajustados

```{r, all-models, fig.cap='(ref:all-models)', fig.width=13, fig.asp=0.6}
autoplot(tune_res, select_best = TRUE) + drako
```

<br/>

En la figura \@ref(fig:all-models) vemos `r nrow(tune_rank)` combinaciones de
los mejores modelos con diferentes motores y algoritmos de balanceo. La
especificidad es la proporción de ceros (nonspam) estimados como ceros, es
decir, que proporción de correos de tipo "no spam" fueron clasificados
correctamente como "no spam".  Esto es relevante dada la preocupación de que un
correo legítimo sea considerado como "spam".

Seleccionemos los dos mejores modelos de cada tipo.  En este caso vamos a
omitir el primer modelo debido a que el coeficiente de Kappa es muy bajo,
al igual que la sensibilidad.

```{r}
(best <- tune_rank %>% 
	slice(2:3) %>% 
	pull(modelo) %>% 
	set_names(.))
```

```{r}
metricas_train <- tune_rank %>% 
	filter(modelo %in% best) 
```

<!-- pred_nonspam .pred_spam -->

Es importante revisar el orden de los niveles en la variable respuesta, ya que
al momento de calcular el AUC debemos especificar que evento es el que queremos
indicar.

```{r}
contrasts(spam$type)
```

Crearemos una función para flexibilizar el gráfico de AUC, aunque no sea
posible (por el momento) generalizarla para los datos de prueba.

```{r}
roc_kear <- function(top_models, wflow, verdad, evto_pred, nivel = "second") {
	
	# tomar en cuenta que `collect_predictions()` puede sumarizar los diversos
	# resultados sobre las predicciones replicadas fuera de la muestra, es decir,
	# los resultados se promedian sobre predicciones repetidas
	.datos <- top_models %>% 
		imap_dfr(~ {{ wflow }} %>% extract_workflow_set_result(id = .x) %>% 
						collect_predictions(), .id = "modelo")
		
		
	# calcular el área bajo la curva para cada modelo	
	auc <- .datos %>%
		group_by(modelo) %>%
		roc_auc({{ verdad }}, {{ evto_pred }}, event_level = nivel) %>%
		select(modelo, auc = .estimate) %>%
		mutate(mo = c("svm", "knn"),
		       por = percent_format(accuracy = 0.0100000000)(auc),
				 .keep = "used") %>%
		unite("nombre", c(mo, por), sep = ": ")
		
	# graficar ROC	
	.datos %>%
		group_by(modelo) %>%
		roc_curve({{ verdad }}, {{ evto_pred }}, event_level = nivel) %>%
		ggplot(aes(x = 1 - specificity, y = sensitivity, color = modelo)) +
		geom_line(size = 1, alpha = 0.5) +
		geom_abline(lty   = 2,
						alpha = 0.5,
						color = "gray50",
						size  = 1.2) +
		annotate("text",
					x = 0.35,
					y = 0.75,
					label = auc$nombre[[1]],
					size  = 10) +
		annotate("text",
					x = 0.35,
					y = 0.63,
					label = auc$nombre[[2]],
					size  = 10) +
		drako + theme(legend.position = c(0.7, 0.1)) +
		labs(title = "Datos de entrenamiento")
	
}
```

Aplicamos la función

(ref:roc) ¿Cómo se ven las curvas de ROC para estos modelos?

```{r, roc, fig.cap='(ref:roc)', fig.width=11, fig.asp=0.7}
roc_train <- roc_kear(
	top_models = best,
	wflow      = tune_res,
	verdad     = type,
	evto_pred  = .pred_spam,
	nivel      = "second")

roc_train
```

<br/>

En la figura \@ref(fig:roc) estamos evaluando la tasa de verdaderos positivos
del evento spam.  Los valores no son tal altos debido a que seleccionamos los
modelos con base a la mayor especificidad, es decir aquellos con mayor capacidad
de seleccionar correctamente aquellos correos que no sean spam.  Para efectos
prácticos analicemos el AUC en materia de capacidad de los modelos de predecir
correctamente los correos spam.

Guardemos nuestros mejores modelos de cada motor en una lista

```{r}
(lista_mejores <- best %>% 
	map(~ tune_res %>% extract_workflow_set_result(id = .x) %>% 
					       select_best(metric = "specificity")))
```

Al tenerlos en una lista nos permitirá realizar iteraciones sobre los mismos
para posteriormente validar resultados con el set de pruebas.

## Hiperparámetros

Habiendo seleccionado los mejores modelos de SVM y de K-NN, procederemos a
visualizar la cuadrícula de hiperparámetros que se ajustaron para encontrar los
valores óptimos.

### Evolución

```{r}
grf <- best %>% imap(~ autoplot(tune_res, select_best = TRUE, id = .x) + 
							drako + labs(title = .y))
```

(ref:svm-cost) Evolución de los hiperparámetros de SVM polinomial

```{r, svm-cost, fig.cap='(ref:svm-cost)', echo=FALSE, fig.width=13, fig.asp=0.6}
grf[[1]]
```

<br/>

Vemos en la gráfica \@ref(fig:svm-cost) como evolucionan los hiperparámetros
del modelo seleccionado en cada una de las métricas previamente establecidas. 
En este caso como tenemos un *kernel polinomial* podemos ver además de la
función de costo $C$ los grados $d$. Entre más grande el grado el límite de
decisión será más flexible.


(ref:knn-cost) Evolución de los hiperparámetros de K-NN

```{r, knn-cost, fig.cap='(ref:knn-cost)', echo=FALSE, fig.width=13, fig.asp=0.6}
grf[[2]]
```

<br/>

En la figura \@ref(fig:knn-cost) vemos los diferentes valores de los hiperparámetros
para el modelo k-nn con ajuste de desbalance por submuestreo. Se observan las
diferentes combinaciones de $k$ con las distintas funciones de distancia. Vemos
a simple vista que para una máxima especificidad tenemos una función
`r coloring_font("**inv**", "#A24000")` y 9 vecinos.

### Información

Veamos de forma independiente el modelo con los datos de entrenamiento y los
hiperparámetros seleccionados posterior al tuning

```{r}
info_models <- best %>% 
	map(~ tune_res %>% 
	extract_workflow(id = .x) %>% 
	finalize_workflow(tune_res %>% 
							extract_workflow_set_result(.x) %>% 
							select_best(metric = "specificity")) %>% 
	fit(spam_train) %>% 
	extract_model())
```

```{r}
(svp <- info_models$smote_outlier_svm_p_kernlab)
```

El primer elemento es un objeto "ksvm" de clase S4 el cual indica lo siguiente:

- La función nos indica que hemos realizado un modelo de clasificación.
- Es un kernel polinómico con grado `r svp@kernelf@kpar$degree`
- Con una función de costo $C$ = `r svp@param$C`
- Tenemos `r svp@nSV` vectores de soporte, 

```{r}
(knd <- info_models$smote_simple_knn)
```

- El mejor fue k = `r knd$best.parameters$k`, el cual es impar.
- La mejor función de distancia ponderada es `r knd$best.parameters$kernel`

## Evaluación

Posterior a esto haremos lo siguiente:

1. Extraer el/los modelos seleccionados del workflow llamado
`r coloring_font("**tune_res**", "#A24000")` 
utilizando la función `extract_workflow()`. Recordemos que un workflow es un
combinación de receta de preprocesamiento en conjunto con un motor (modelo) a
utilizar. Los workflows seleccionado serán aquellos que con base a métricas
revisadas en el paso anterior se consideran óptimos.

2. Con la función `finalize_workflow()` lo que haremos será decirle al workflow
seleccionado que usaremos uno los modelos ajustados con datos de entrenamiento
que contiene los mejores parámetros numéricos encontrados a través del tuning
usando CV.

3. Con `last_fit()` realizaremos el ajuste final tanto a los datos prueba usando
los modelos previamente seleccionados. Debemos asegurarnos de definir el set
de métricas que deseamos comprobar, pudiendo agregar nuevas métricas.

```{r}
test_result_list <- map2(.x = best, .y = lista_mejores, ~ tune_res %>% 
	extract_workflow(id = .x) %>% 
	finalize_workflow(.y) %>% 
	last_fit(split = spam_split, metrics = mset))
```

En la iteración por pares con `map2()` hacemos que el primer modelo que se
encuentra en el objeto `r coloring_font("**best**", "#A24000")` se finalice
con el primer modelo contenido en `r coloring_font("**lista_mejores**", "#A24000")`.
Esta operación garantiza que se realicen los ajustes de los modelos óptimos
seleccionados al conjunto de validación.

```{r}
test_result_list
```

En esta lista vemos los resultados de los modelos seleccionados ajustados a los
datos de validación.

4. Haciendo uso de `collect_metrics()` podemos ver las métricas para el conjunto
de prueba

```{r}
metricas_test <- test_result_list %>% 
	map_dfr(~ collect_metrics(.x), .id = "modelo") %>% 
	pivot_wider(names_from = .metric, values_from = .estimate) %>%
	select(-c(.estimator:.config)) %>% 
	rename(kappa = kap) %>% 
	relocate(specificity, roc_auc, kappa, .after = modelo)
```

```{r}
metricas_test %>% 
	tabla("Desempeño de los modelos con los datos de validación")
```

<br/>

5. Con `collect_predictions()` podemos ver cómo podemos esperar que este modelo
funcione con nuevos datos.

Primero calculemos el AUC utilizando la función `roc_auc()` en la que definimos
que el evento verdad (*truth*) es la columna *type* y la estimación de
probabilidad numérica es  `r coloring_font("**.pred_spam**", "blue")`

```{r, paged.print = FALSE}
(auc_test <- test_result_list %>% 
	map_dfr(~ collect_predictions(.x), .id = "modelo") %>% 
	group_by(modelo) %>% 
	roc_auc(type, .pred_spam, event_level = "second") %>% 
	select(modelo, auc = .estimate) %>%
	mutate(mo = c("svm", "knn"),
		    por = percent_format(accuracy = 0.0100000000)(auc),
			.keep = "used") %>%
	unite("nombre", c(mo, por), sep = ": "))
```

(ref:roc-test) ¿Cómo se ven las curvas de ROC para estos modelos en los datos de prueba?

```{r, roc-test, fig.cap='(ref:roc-test)', fig.width=13, fig.asp=0.7}
roc_test <- test_result_list %>% 
	map_dfr(~ collect_predictions(.x), .id = "modelo") %>% 
	group_by(modelo) %>% 
    roc_curve(type, .pred_spam, event_level = "second") %>%  
	ggplot(aes(x = 1 - specificity, y = sensitivity, color = modelo)) +
		geom_line(size = 1, alpha = 0.5) +
		geom_abline(lty   = 2,
						alpha = 0.5,
						color = "gray50",
						size  = 1.2) +
		annotate("text",
					x = 0.35,
					y = 0.75,
					label = auc_test$nombre[[1]],
					size  = 10) +
		annotate("text",
					x = 0.35,
					y = 0.63,
					label = auc_test$nombre[[2]],
					size  = 10) +
		drako + theme(legend.position = c(0.7, 0.1)) +
		labs(title = "Datos de prueba")
	
```

(ref:idf) ¿Cuál de los dos modelos podemos considerar que es mejor en función del AUC?

```{r, idf, fig.cap='(ref:idf)', fig.width=13, fig.asp=0.7}
list(roc_train, roc_test) %>% 
	reduce(.f = `+`) +
	plot_layout(ncol = 2) +
    plot_annotation(title = "Diferencias entre AUC para conjuntos de entrenamiento y prueba")
```

En el gráfico \@ref(fig:idf) vemos que el AUC del modelo KNN en el conjunto de
prueba es más alto. Vemos que las métricas del conjunto de prueba son
**ligeramente** superiores a las del conjunto de entrenamiento.

### Matriz de Confusión

```{r}
test_result_list %>% 
	map_dfr(~ collect_predictions(.x), .id = "modelo") %>% 
	group_by(modelo) %>% 
	conf_mat(type, .pred_class) %>% 
	rename(mc = conf_mat) %>% 
	mutate(mc = set_names(mc, c("svm", "knn")))


	mutate(graficos = map(conf_mat, ~ autoplot(.x) + )) 
	# mutate(graficos = map(conf_mat, ~ autoplot(.x))) 

%$%
			 graficos %>% 
				set_names(names(test_result_list)) %>% 
				iwalk(~ print(.x))
			 graficos %>% walk(~ print(.x)) %>% 
```

### Métricas

```{r}
comparacion <- metricas_train %>%
	mutate(datos = "entrenamiento", .after = modelo) %>%
	select(-rank) %>%
	bind_rows(metricas_test %>%
	mutate(datos = "prueba", .after = modelo))
```

```{r}
comparacion[, 3:7] <- map(comparacion %>%
	 	select(is.numeric), ~ color_tile("#FC8D59", "lightgreen")(.x))
```

```{r, comp}
comparacion %>%
	tabla(cap = "Comparacion de métricas")
```

<br/>

En la tabla \@ref(tab:comp) vemos en verde el que obtuvo mayor valor con
respecto a la métrica de esa columna.  En cuatro de las cinco métricas el
modelo `r coloring_font("**smote_simple_knn**", "#AFC8D59")` del conjunto de
validación (prueba) obtuvo mejores resultados.

Veamos una forma diferente de analizar estas métricas:

```{r, comp2}
metricas_train %>% 
	select(-rank) %>% 
	pivot_longer(cols = specificity:sensitivity,
					 names_to = "metrica",
					 values_to = "train") %>% 
	bind_cols(metricas_test %>% 
				 	pivot_longer(cols = specificity:sensitivity,
					 names_to = "metrica",
					 values_to = "test") %>% 
				 	select(test)) %>% 
	mutate(delta = test - train, 
			 diff_porcentual = percent_format(accuracy = 0.01)(delta)) %>% 
	tabla(cap = "Métricas de Train contra test")
```

<br/>

En la tabla \@ref(tab:comp2) podemos ver lo siguiente:

* Los valores positivos de la columna `r coloring_font("**delta**", "#A24000")`
indican que la métrica de **test fue superior a la de train.**.

* La columna `r coloring_font("**diff_porcentual**", "#A24000")` representa que
tanto porcentualmente son diferentes las métricas de train con respecto a test.

* Para `r coloring_font("**smote_simple_knn**", "#A24000")` tenemos que
$\frac{3}{5}$ de las métricas de test fueron superiores a las de train.

* Para `r coloring_font("**smote_outlier_svm_p_kernlab**", "#A24000")` tenemos
que $\frac{2}{5}$ de las métricas de test fueron superiores a las de train.




# Modelo Definitivo

Una vez que tenemos el modelo correcto, con los parámetros seleccionados, volveremos
que volver a entrenarlo con toda la base disponible (train + test) obteniendo el
modelo definitivo que puedes poner en producción


# Referencias
